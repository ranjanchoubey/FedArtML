{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80ba121",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ranjanchoubey/FedArtML/blob/main/examples/00_InSDN_Data_Preprocessing_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e69c1",
   "metadata": {},
   "source": [
    "# InSDN Dataset: Comprehensive Preprocessing & Exploratory Data Analysis\n",
    "\n",
    "This notebook provides an in-depth exploratory data analysis (EDA) and preprocessing of the InSDN (Software-Defined Network Intrusion Detection) dataset. We cover data loading, cleaning, feature engineering, visualization, and statistical analysis to prepare data for federated learning.\n",
    "\n",
    "**Dataset Overview:**\n",
    "- **Total Records:** ~138,000 network flow records\n",
    "- **Features:** 80+ network flow characteristics\n",
    "- **Classes:** 6 attack/traffic types (DoS, DDoS, Probe, BFA, BOTNET, Web-Attack)\n",
    "- **Source:** https://aseados.ucd.ie/datasets/SDN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup: Import Libraries & Configuration { display-mode: \"form\" }\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing and statistics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Color palette\n",
    "colors = [\"#00cfcc\", \"#e6013b\", \"#007f88\", \"#00cccd\", \"#69e0da\", \"darkblue\", \"#ff6b6b\"]\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fafef",
   "metadata": {},
   "source": [
    "# Section 1: Load and Explore InSDN Dataset\n",
    "\n",
    "Loading the InSDN dataset and performing initial exploratory analysis to understand data structure, dimensions, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d952bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load InSDN Dataset { display-mode: \"form\" }\n",
    "\n",
    "dataset_path = '../data/LINK_all_features_all_datsets/InSDN'\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING InSDN DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = None\n",
    "\n",
    "# Option 1: Try to load from local path\n",
    "try:\n",
    "    if os.path.exists(dataset_path):\n",
    "        data_files = [f for f in os.listdir(dataset_path) if f.endswith('.csv')]\n",
    "        if data_files:\n",
    "            print(f\"\\n‚úì Found local dataset: {data_files[0]}\")\n",
    "            df = pd.read_csv(os.path.join(dataset_path, data_files[0]))\n",
    "except Exception as e:\n",
    "    print(f\"Local path check failed: {e}\")\n",
    "\n",
    "# Option 2: Download from official source\n",
    "if df is None:\n",
    "    try:\n",
    "        print(\"\\n‚úì Downloading InSDN dataset from UCD...\")\n",
    "        zip_url = \"https://aseados.ucd.ie/datasets/SDN/InSDN_DatasetCSV.zip\"\n",
    "        response = requests.get(zip_url, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"  Download successful! Extracting...\")\n",
    "            zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "            csv_files = [f for f in zip_file.namelist() if f.endswith('.csv')]\n",
    "            selected_file = next((f for f in csv_files if 'OVS.csv' in f), csv_files[0])\n",
    "            print(f\"  Loading: {selected_file}\")\n",
    "            df = pd.read_csv(zip_file.open(selected_file))\n",
    "            print(f\"‚úì Successfully loaded! Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "\n",
    "if df is None or df.empty:\n",
    "    print(\"‚ö† Could not load dataset. Please ensure it's available.\")\n",
    "else:\n",
    "    # Display basic information\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATASET INFORMATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nShape: {df.shape[0]:,} records √ó {df.shape[1]} features\")\n",
    "    print(f\"\\nData Types:\\n{df.dtypes.value_counts()}\")\n",
    "    print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Column information\n",
    "    print(f\"\\nColumn Names ({len(df.columns)} total):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Label Distribution Analysis { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LABEL DISTRIBUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    label_col = 'Label'\n",
    "    if label_col in df.columns:\n",
    "        label_counts = df[label_col].value_counts()\n",
    "        label_pct = 100 * df[label_col].value_counts(normalize=True)\n",
    "        \n",
    "        print(f\"\\nAttack Types and Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            pct = label_pct[label]\n",
    "            print(f\"  {label:15s}: {count:7,d} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nClass Imbalance Ratio (Max/Min): {label_counts.max() / label_counts.min():.2f}\")\n",
    "        \n",
    "        # Visualize label distribution\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Bar plot\n",
    "        label_counts.plot(kind='bar', ax=axes[0], color=colors[0], alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title('Attack Type Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Attack Type', fontsize=12)\n",
    "        axes[0].set_ylabel('Count', fontsize=12)\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Pie chart\n",
    "        axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',\n",
    "                   colors=colors, startangle=90)\n",
    "        axes[1].set_title('Attack Type Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80c813",
   "metadata": {},
   "source": [
    "# Section 2: Handle Missing Values and Outliers\n",
    "\n",
    "Identify missing values, implement imputation strategies, and detect/handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Missing Values & Imputation Analysis { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check missing values\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing Count': df.isnull().sum(),\n",
    "        'Missing Percentage': 100 * df.isnull().sum() / len(df)\n",
    "    })\n",
    "    missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "    \n",
    "    if len(missing_data) > 0:\n",
    "        print(\"\\nMissing Values Found:\")\n",
    "        print(missing_data.to_string(index=False))\n",
    "        \n",
    "        # Visualize missing values\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        missing_cols = missing_data.head(15)\n",
    "        ax.barh(missing_cols['Column'], missing_cols['Missing Percentage'], color=colors[1], alpha=0.7)\n",
    "        ax.set_xlabel('Missing Percentage (%)', fontsize=12)\n",
    "        ax.set_title('Top 15 Columns with Missing Values', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚úì No missing values detected!\")\n",
    "    \n",
    "    # Separate numeric and non-numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nNumeric Columns: {len(numeric_cols)}\")\n",
    "    print(f\"Non-Numeric Columns: {len(non_numeric_cols)}\")\n",
    "    \n",
    "    # Imputation if needed\n",
    "    if missing_data.shape[0] > 0:\n",
    "        print(\"\\n‚úì Applying mean imputation for numeric columns...\")\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        df_numeric = df[numeric_cols].copy()\n",
    "        df_numeric = pd.DataFrame(imputer.fit_transform(df_numeric), columns=numeric_cols)\n",
    "        print(\"‚úì Imputation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3015e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Outlier Detection & IQR Analysis { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OUTLIER DETECTION & ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate outliers for numeric columns\n",
    "    outlier_counts = {}\n",
    "    for col in numeric_cols[:10]:  # Check first 10 numeric columns for visualization\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_counts[col] = len(outliers)\n",
    "    \n",
    "    print(\"\\nOutlier Counts (IQR method, first 10 numeric columns):\")\n",
    "    for col, count in sorted(outlier_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        pct = 100 * count / len(df)\n",
    "        print(f\"  {col:20s}: {count:7,d} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Visualize outliers with box plots\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols[:10]):\n",
    "        axes[idx].boxplot(df[col], vert=True)\n",
    "        axes[idx].set_title(f'Boxplot: {col}', fontsize=10, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Value', fontsize=9)\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Outlier Detection: Box Plots (First 10 Numeric Features)', \n",
    "                 fontsize=14, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Box plots show potential outliers (dots beyond whiskers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e645f7",
   "metadata": {},
   "source": [
    "# Section 3: Feature Scaling and Normalization\n",
    "\n",
    "Compare different scaling methods and visualize their effects on feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Feature Scaling & Normalization Comparison { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE SCALING & NORMALIZATION COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Select sample features for visualization\n",
    "    sample_features = numeric_cols[:3]\n",
    "    \n",
    "    # Apply different scalers\n",
    "    df_original = df[sample_features].copy()\n",
    "    \n",
    "    scaler_standard = StandardScaler()\n",
    "    df_standard = pd.DataFrame(scaler_standard.fit_transform(df_original), columns=sample_features)\n",
    "    \n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    df_minmax = pd.DataFrame(scaler_minmax.fit_transform(df_original), columns=sample_features)\n",
    "    \n",
    "    scaler_robust = RobustScaler()\n",
    "    df_robust = pd.DataFrame(scaler_robust.fit_transform(df_original), columns=sample_features)\n",
    "    \n",
    "    # Visualize scaling comparison\n",
    "    fig, axes = plt.subplots(len(sample_features), 4, figsize=(18, 12))\n",
    "    \n",
    "    for idx, feature in enumerate(sample_features):\n",
    "        # Original\n",
    "        axes[idx, 0].hist(df_original[feature], bins=50, color=colors[0], alpha=0.7, edgecolor='black')\n",
    "        axes[idx, 0].set_title(f'{feature}\\n(Original)', fontsize=10, fontweight='bold')\n",
    "        axes[idx, 0].set_ylabel('Frequency', fontsize=9)\n",
    "        axes[idx, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Standard Scaler\n",
    "        axes[idx, 1].hist(df_standard[feature], bins=50, color=colors[2], alpha=0.7, edgecolor='black')\n",
    "        axes[idx, 1].set_title(f'{feature}\\n(StandardScaler)', fontsize=10, fontweight='bold')\n",
    "        axes[idx, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # MinMax Scaler\n",
    "        axes[idx, 2].hist(df_minmax[feature], bins=50, color=colors[3], alpha=0.7, edgecolor='black')\n",
    "        axes[idx, 2].set_title(f'{feature}\\n(MinMaxScaler)', fontsize=10, fontweight='bold')\n",
    "        axes[idx, 2].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Robust Scaler\n",
    "        axes[idx, 3].hist(df_robust[feature], bins=50, color=colors[4], alpha=0.7, edgecolor='black')\n",
    "        axes[idx, 3].set_title(f'{feature}\\n(RobustScaler)', fontsize=10, fontweight='bold')\n",
    "        axes[idx, 3].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Feature Scaling Comparison: Distribution Before & After', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply StandardScaler to all numeric columns for further analysis\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_cols]), columns=numeric_cols)\n",
    "    \n",
    "    print(\"\\n‚úì StandardScaler applied to all numeric features\")\n",
    "    print(f\"  Scaled data shape: {df_scaled.shape}\")\n",
    "    print(f\"  Mean (should be ~0): {df_scaled.mean().mean():.6f}\")\n",
    "    print(f\"  Std Dev (should be ~1): {df_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4950b3",
   "metadata": {},
   "source": [
    "# Section 4: Univariate Analysis with Visualizations\n",
    "\n",
    "Analyze individual feature distributions, skewness, kurtosis, and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03280ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Univariate Analysis: Feature Distributions { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"UNIVARIATE ANALYSIS: FEATURE DISTRIBUTIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\nStatistical Summary (First 10 Numeric Features):\")\n",
    "    stats_summary = df[numeric_cols[:10]].describe().T\n",
    "    stats_summary['skewness'] = df[numeric_cols[:10]].skew()\n",
    "    stats_summary['kurtosis'] = df[numeric_cols[:10]].kurtosis()\n",
    "    print(stats_summary.to_string())\n",
    "    \n",
    "    # Histograms and KDE plots\n",
    "    fig, axes = plt.subplots(5, 4, figsize=(20, 18))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols[:20]):\n",
    "        # Histogram\n",
    "        axes[idx].hist(df[col], bins=50, color=colors[0], alpha=0.6, edgecolor='black', label='Histogram')\n",
    "        \n",
    "        # KDE overlay\n",
    "        ax2 = axes[idx].twinx()\n",
    "        df[col].plot(kind='kde', ax=ax2, color=colors[5], linewidth=2.5, label='KDE')\n",
    "        ax2.set_ylabel('Density', fontsize=9)\n",
    "        ax2.legend(loc='upper right', fontsize=8)\n",
    "        \n",
    "        axes[idx].set_title(f'{col}\\n(Skew: {df[col].skew():.2f}, Kurt: {df[col].kurtosis():.2f})', \n",
    "                           fontsize=9, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Value', fontsize=8)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=8)\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        axes[idx].tick_params(labelsize=7)\n",
    "    \n",
    "    plt.suptitle('Univariate Analysis: Distribution, Skewness & Kurtosis (First 20 Features)', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Feature Distributions by Attack Type { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty and 'Label' in df.columns:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE DISTRIBUTIONS BY ATTACK TYPE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Select top 6 numeric features for visualization\n",
    "    top_features = numeric_cols[:6]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        for label in df['Label'].unique():\n",
    "            data = df[df['Label'] == label][feature]\n",
    "            axes[idx].hist(data, bins=40, alpha=0.5, label=label, edgecolor='black')\n",
    "        \n",
    "        axes[idx].set_title(f'Distribution by Attack Type: {feature}', fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Value', fontsize=10)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "        axes[idx].legend(fontsize=8, loc='upper right')\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Univariate Analysis: Feature Distributions by Attack Type', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be24275",
   "metadata": {},
   "source": [
    "# Section 5: Bivariate and Multivariate Analysis\n",
    "\n",
    "Analyze relationships between feature pairs and perform dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Bivariate Analysis: Feature Relationships { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BIVARIATE ANALYSIS: SCATTER PLOTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Scatter plots for feature pairs\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    feature_pairs = [\n",
    "        (numeric_cols[0], numeric_cols[1]),\n",
    "        (numeric_cols[1], numeric_cols[2]),\n",
    "        (numeric_cols[2], numeric_cols[3]),\n",
    "        (numeric_cols[3], numeric_cols[4]),\n",
    "        (numeric_cols[4], numeric_cols[5]),\n",
    "        (numeric_cols[5], numeric_cols[6])\n",
    "    ]\n",
    "    \n",
    "    for idx, (feat1, feat2) in enumerate(feature_pairs):\n",
    "        axes[idx].scatter(df[feat1], df[feat2], alpha=0.5, s=10, color=colors[0])\n",
    "        axes[idx].set_xlabel(feat1, fontsize=10)\n",
    "        axes[idx].set_ylabel(feat2, fontsize=10)\n",
    "        axes[idx].set_title(f'{feat1} vs {feat2}', fontsize=11, fontweight='bold')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Bivariate Analysis: Scatter Plots of Feature Pairs', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Multivariate Analysis: PCA Dimensionality Reduction { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MULTIVARIATE ANALYSIS: PCA DIMENSIONALITY REDUCTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(df_scaled)\n",
    "    \n",
    "    print(f\"\\nExplained Variance Ratio:\")\n",
    "    print(f\"  PC1: {pca.explained_variance_ratio_[0]:.4f} ({100*pca.explained_variance_ratio_[0]:.2f}%)\")\n",
    "    print(f\"  PC2: {pca.explained_variance_ratio_[1]:.4f} ({100*pca.explained_variance_ratio_[1]:.2f}%)\")\n",
    "    print(f\"  Total: {sum(pca.explained_variance_ratio_):.4f} ({100*sum(pca.explained_variance_ratio_):.2f}%)\")\n",
    "    \n",
    "    # Visualize PCA results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # PCA scatter by attack type\n",
    "    if 'Label' in df.columns:\n",
    "        label_colors = {label: colors[i % len(colors)] for i, label in enumerate(df['Label'].unique())}\n",
    "        for label in df['Label'].unique():\n",
    "            mask = df['Label'] == label\n",
    "            axes[0].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                          label=label, alpha=0.6, s=20, color=label_colors[label])\n",
    "        axes[0].set_xlabel(f'PC1 ({100*pca.explained_variance_ratio_[0]:.1f}%)', fontsize=12)\n",
    "        axes[0].set_ylabel(f'PC2 ({100*pca.explained_variance_ratio_[1]:.1f}%)', fontsize=12)\n",
    "        axes[0].set_title('PCA: Colored by Attack Type', fontsize=13, fontweight='bold')\n",
    "        axes[0].legend(fontsize=10, loc='best')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Scree plot\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(df_scaled)\n",
    "    cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    axes[1].plot(range(1, len(cumsum)+1), cumsum, 'bo-', linewidth=2, markersize=6)\n",
    "    axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "    axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "    axes[1].set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "    axes[1].set_title('PCA Scree Plot', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].set_xlim([1, 50])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find number of components for 95% variance\n",
    "    n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "    print(f\"\\n‚úì Number of components needed for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680410b",
   "metadata": {},
   "source": [
    "# Section 6: Class Distribution and Imbalance Analysis\n",
    "\n",
    "Detailed analysis of class imbalance and minority class characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Class Imbalance Analysis { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty and 'Label' in df.columns:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CLASS IMBALANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    label_dist = df['Label'].value_counts().sort_values(ascending=False)\n",
    "    label_pct = 100 * df['Label'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "    \n",
    "    imbalance_ratio = label_dist.max() / label_dist.min()\n",
    "    print(f\"\\nClass Imbalance Ratio (Max/Min): {imbalance_ratio:.2f}\")\n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    for label, count in label_dist.items():\n",
    "        pct = label_pct[label]\n",
    "        print(f\"  {label:15s}: {count:7,d} ({pct:6.2f}%) {'‚ñà' * int(pct/2)}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'bar'}, {'type': 'pie'}]])\n",
    "    \n",
    "    # Bar chart\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=label_dist.index, y=label_dist.values, marker_color=colors[:len(label_dist)],\n",
    "               text=label_dist.values, textposition='auto', name='Count'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.update_xaxes(title_text='Attack Type', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "    \n",
    "    # Pie chart\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=label_dist.index, values=label_dist.values, name='Percentage'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(title_text='Class Distribution and Imbalance Analysis', \n",
    "                     height=500, showlegend=True)\n",
    "    fig.show()\n",
    "    \n",
    "    # Stacked bar by data split\n",
    "    print(\"\\n‚úì Class distribution analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d25025",
   "metadata": {},
   "source": [
    "# Federated Data Split - Client Analysis\n",
    "\n",
    "This section analyzes how the data is distributed across different federated clients using the FedArtML library with Dirichlet distribution (label skew).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Federated Learning Configuration { display-mode: \"form\" }\n",
    "\n",
    "# Number of Federated Clients\n",
    "num_clients = 5  # @param {type: \"integer\", min: 2, max: 50, step: 1}\n",
    "\n",
    "# Distribution Method\n",
    "distribution_method = \"dirichlet\"  # @param [\"dirichlet\", \"percent_noniid\", \"no-label-skew\"]\n",
    "\n",
    "# Dirichlet Alpha Parameter (Heterogeneity Control)\n",
    "# - Lower values (0.1-0.5): More heterogeneous/non-IID data distribution\n",
    "# - Higher values (1-10): More uniform/IID data distribution\n",
    "alpha = 1.0  # @param {type: \"number\", min: 0.1, max: 10.0, step: 0.1}\n",
    "\n",
    "# Random State (for reproducibility)\n",
    "random_state = 42  # @param {type: \"integer\", min: 0, max: 10000, step: 1}\n",
    "\n",
    "# Data Completeness\n",
    "data_completeness = \"without_class_completion\"  # @param [\"without_class_completion\", \"with_class_completion\"]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEDERATED LEARNING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n  üìä Number of Clients: {num_clients}\")\n",
    "print(f\"  üîÄ Distribution Method: {distribution_method.upper()}\")\n",
    "print(f\"  üîÄ Dirichlet Alpha (Heterogeneity): {alpha}\")\n",
    "print(f\"  üé≤ Random State (Seed): {random_state}\")\n",
    "print(f\"  ‚úì Data Completeness: {data_completeness}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìù PARAMETER DESCRIPTIONS:\")\n",
    "print(f\"\\n  num_clients:\")\n",
    "print(f\"    ‚Üí Number of federated clients to split data across\")\n",
    "print(f\"    ‚Üí Range: 2-50 clients\")\n",
    "\n",
    "print(f\"\\n  distribution_method:\")\n",
    "print(f\"    Available methods: ['dirichlet', 'percent_noniid', 'no-label-skew']\")\n",
    "print(f\"\\n    ‚îú‚îÄ 'dirichlet' (RECOMMENDED for heterogeneous data):\")\n",
    "print(f\"    ‚îÇ  ‚Ä¢ Uses Dirichlet distribution for controlled label skew\")\n",
    "print(f\"    ‚îÇ  ‚Ä¢ Alpha parameter controls degree of heterogeneity\")\n",
    "print(f\"    ‚îÇ  ‚Ä¢ Best for realistic federated learning scenarios\")\n",
    "print(f\"    ‚îÇ  ‚Ä¢ Simulates real-world non-IID data distribution\")\n",
    "print(f\"\\n    ‚îú‚îÄ 'percent_noniid' (For percentage-based heterogeneity):\")\n",
    "print(f\"    ‚îÇ  ‚Ä¢ Creates non-IID data by specifying percentage of non-IID samples\")\n",
    "print(f\"    ‚îÇ  ‚Ä¢ Useful for controlled heterogeneity testing\")\n",
    "print(f\"    ‚îÇ  ‚Ä¢ Alternative approach to label skew\")\n",
    "print(f\"\\n    ‚îî‚îÄ 'no-label-skew' (For IID baseline experiments):\")\n",
    "print(f\"       ‚Ä¢ Creates Independent and Identically Distributed data\")\n",
    "print(f\"       ‚Ä¢ Each client gets random samples from all classes\")\n",
    "print(f\"       ‚Ä¢ Use for baseline/control experiments\")\n",
    "print(f\"       ‚Ä¢ No heterogeneity - uniform distribution across clients\")\n",
    "\n",
    "print(f\"\\n  alpha:\")\n",
    "print(f\"    ‚Üí Dirichlet concentration parameter (controls data heterogeneity)\")\n",
    "print(f\"    ‚Üí ONLY USED when method='dirichlet'\")\n",
    "print(f\"    ‚Üí Œ± < 0.5: Highly heterogeneous (extreme non-IID)\")\n",
    "print(f\"    ‚Üí Œ± = 1.0: Moderate heterogeneity ‚úì DEFAULT\")\n",
    "print(f\"    ‚Üí Œ± > 5.0: Nearly uniform (near IID)\")\n",
    "\n",
    "print(f\"\\n  random_state:\")\n",
    "print(f\"    ‚Üí Seed value for reproducibility\")\n",
    "print(f\"    ‚Üí Same seed = same data split across runs\")\n",
    "\n",
    "print(f\"\\n  data_completeness:\")\n",
    "print(f\"    ‚Üí {data_completeness}: Clients may not have all classes\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì CURRENT CONFIGURATION: {distribution_method.upper()} distribution\")\n",
    "print(f\"  Ready to create {num_clients} federated clients with Œ±={alpha}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Federated Data Split { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty and 'Label' in df.columns:\n",
    "    from fedartml import SplitAsFederatedData\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"CREATING FEDERATED DATA SPLIT ({distribution_method.upper()} DISTRIBUTION)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Use parameters from the configuration cell above\n",
    "    print(f\"\\n‚úì Federation Parameters from Configuration:\")\n",
    "    print(f\"  ‚Ä¢ Number of Clients: {num_clients}\")\n",
    "    print(f\"  ‚Ä¢ Distribution Method: {distribution_method.upper()}\")\n",
    "    \n",
    "    # Method-specific parameters\n",
    "    if distribution_method == \"dirichlet\":\n",
    "        print(f\"  ‚Ä¢ Alpha (heterogeneity): {alpha}\")\n",
    "        print(f\"    ‚îî‚îÄ Creating label-skewed distribution using Dirichlet\")\n",
    "    elif distribution_method == \"percent_noniid\":\n",
    "        print(f\"  ‚Ä¢ Method: Percent Non-IID\")\n",
    "        print(f\"    ‚îî‚îÄ Creating percentage-based heterogeneous distribution\")\n",
    "    elif distribution_method == \"no-label-skew\":\n",
    "        print(f\"  ‚Ä¢ Method: No Label Skew (IID)\")\n",
    "        print(f\"    ‚îî‚îÄ Creating uniform/IID distribution across clients\")\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Random State: {random_state}\")\n",
    "    print(f\"  ‚Ä¢ Data Completeness: {data_completeness}\")\n",
    "    print(f\"\\nNote: Lower alpha values create higher label skew (non-IID) across clients\")\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    y_labels = df['Label'].values\n",
    "    unique_labels = sorted(set(y_labels))\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
    "    y = np.array([label_mapping[label] for label in y_labels]).astype(int)\n",
    "    \n",
    "    # Select only numeric features\n",
    "    X_df = df.drop(columns=['Label', 'Flow ID', 'Src IP', 'Dst IP', 'Timestamp'], errors='ignore')\n",
    "    X_df = X_df.select_dtypes(include=[np.number])\n",
    "    feature_names = X_df.columns.tolist()\n",
    "    X = X_df.values\n",
    "    \n",
    "    # Handle missing values and normalize\n",
    "    print(f\"\\n‚úì Preprocessing Data:\")\n",
    "    print(f\"  ‚Ä¢ Features: {X.shape[1]} numeric features selected\")\n",
    "    print(f\"  ‚Ä¢ Samples: {X.shape[0]} total samples\")\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "    print(f\"  ‚Ä¢ Missing Values: Handled with mean imputation\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    print(f\"  ‚Ä¢ Scaling: StandardScaler applied\")\n",
    "    \n",
    "    # Create federated dataset with selected method\n",
    "    print(f\"\\n‚úì Creating Federated Clients...\")\n",
    "    federater = SplitAsFederatedData(random_state=random_state)\n",
    "    \n",
    "    # Call create_clients with selected distribution method\n",
    "    clients_dict, _, _, distances = federater.create_clients(\n",
    "        image_list=X,\n",
    "        label_list=y,\n",
    "        num_clients=num_clients,\n",
    "        prefix_cli='Client',\n",
    "        method=distribution_method,\n",
    "        alpha=alpha if distribution_method == \"dirichlet\" else None\n",
    "    )\n",
    "    \n",
    "    clients_data = clients_dict[data_completeness]\n",
    "    dist_without = distances[data_completeness]\n",
    "    \n",
    "    # Extract label information for analysis\n",
    "    all_labels_list = sorted(unique_labels)\n",
    "    client_names_list = sorted(clients_data.keys())\n",
    "    client_label_matrix = []\n",
    "    \n",
    "    for client_name in client_names_list:\n",
    "        client_data = clients_data[client_name]\n",
    "        client_labels = [item[1] for item in client_data]\n",
    "        label_counts = []\n",
    "        for label in all_labels_list:\n",
    "            count = sum([1 for lbl in client_labels if int(lbl) == label_mapping[label]])\n",
    "            label_counts.append(count)\n",
    "        client_label_matrix.append(label_counts)\n",
    "    \n",
    "    print(f\"\\n‚úì Federated Split Complete!\")\n",
    "    print(f\"  ‚Ä¢ Clients created: {len(clients_data)}\")\n",
    "    print(f\"  ‚Ä¢ Total data points distributed: {sum(len(v) for v in clients_data.values()):,}\")\n",
    "    print(f\"  ‚Ä¢ Distribution Method: {distribution_method.upper()}\")\n",
    "    \n",
    "    # Display method-specific metrics\n",
    "    if distribution_method == \"dirichlet\":\n",
    "        print(f\"  ‚Ä¢ Non-IID Metrics:\")\n",
    "        print(f\"    ‚îî‚îÄ Jensen-Shannon Distance: {dist_without['jensen-shannon']:.6f}\")\n",
    "        print(f\"    ‚îî‚îÄ Hellinger Distance: {dist_without['hellinger']:.6f}\")\n",
    "        print(f\"    ‚îî‚îÄ Earth Mover's Distance: {dist_without['earth-movers']:.6f}\")\n",
    "    elif distribution_method == \"no-label-skew\":\n",
    "        print(f\"  ‚Ä¢ Distribution: IID (Independent and Identically Distributed)\")\n",
    "        print(f\"    ‚îî‚îÄ Each client has similar class distribution\")\n",
    "        print(f\"    ‚îî‚îÄ Suitable for baseline/control experiments\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a08ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Federated Client Distribution Analysis { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty and 'Label' in df.columns and 'clients_data' in dir():\n",
    "    # Visualization 1: Class Distribution per Client\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Federated Client Data Analysis - Label Distribution', fontsize=18, fontweight='bold', y=1.00)\n",
    "    \n",
    "    # 1. Stacked Bar Chart - Absolute counts\n",
    "    ax1 = axes[0, 0]\n",
    "    bottom = np.zeros(len(client_names_list))\n",
    "    colors_list = plt.cm.Set3(np.linspace(0, 1, len(all_labels_list)))\n",
    "    \n",
    "    for i, label in enumerate(all_labels_list):\n",
    "        counts = [row[i] for row in client_label_matrix]\n",
    "        ax1.bar(client_names_list, counts, bottom=bottom, label=label, color=colors_list[i], edgecolor='black', linewidth=1.5)\n",
    "        bottom += counts\n",
    "    \n",
    "    ax1.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Absolute Sample Distribution per Client', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(title='Attack Type', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Stacked Bar Chart - Percentages\n",
    "    ax2 = axes[0, 1]\n",
    "    bottom = np.zeros(len(client_names_list))\n",
    "    \n",
    "    for i, label in enumerate(all_labels_list):\n",
    "        counts = [row[i] for row in client_label_matrix]\n",
    "        totals = [sum(row) for row in client_label_matrix]\n",
    "        percentages = [100 * c / t if t > 0 else 0 for c, t in zip(counts, totals)]\n",
    "        ax2.bar(client_names_list, percentages, bottom=bottom, label=label, color=colors_list[i], edgecolor='black', linewidth=1.5)\n",
    "        bottom += percentages\n",
    "    \n",
    "    ax2.set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Percentage Distribution per Client', fontsize=13, fontweight='bold')\n",
    "    ax2.legend(title='Attack Type', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Grouped Bar Chart - Label-wise comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    x = np.arange(len(all_labels_list))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, client_name in enumerate(client_names_list):\n",
    "        totals = sum(client_label_matrix[i])\n",
    "        percentages = [100 * c / totals if totals > 0 else 0 for c in client_label_matrix[i]]\n",
    "        ax3.bar(x + i * width, percentages, width, label=client_name, alpha=0.8)\n",
    "    \n",
    "    ax3.set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Attack Type', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Per-Label Comparison Across Clients', fontsize=13, fontweight='bold')\n",
    "    ax3.set_xticks(x + width)\n",
    "    ax3.set_xticklabels(all_labels_list, rotation=45, ha='right')\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Sample Count Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    sample_counts = [sum(row) for row in client_label_matrix]\n",
    "    bars = ax4.barh(client_names_list, sample_counts, color=['#0091ad', '#ff6b6b', '#4ecdc4'], edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, sample_counts)):\n",
    "        ax4.text(count + 500, i, f'{count:,}', va='center', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax4.set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Total Samples per Client', fontsize=13, fontweight='bold')\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Client data distribution visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0f68d",
   "metadata": {},
   "source": [
    "## Detailed Client Feature Statistics\n",
    "\n",
    "Analysis of feature statistics for each federated client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61afc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Client Feature Statistics Analysis { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty and 'Label' in df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED FEATURE STATISTICS BY CLIENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select top features by variance for analysis\n",
    "    feature_names = X_df.columns.tolist()\n",
    "    feature_variance = np.var(X, axis=0)\n",
    "    top_features_idx = np.argsort(feature_variance)[-10:]  # Top 10 features by variance\n",
    "    top_features = [feature_names[i] for i in top_features_idx]\n",
    "    \n",
    "    print(f\"\\nTop 10 Features by Variance:\")\n",
    "    for i, (idx, var) in enumerate(zip(top_features_idx, sorted(feature_variance)[:-11:-1]), 1):\n",
    "        print(f\"  {i:2d}. {feature_names[idx]:30s}: Variance = {var:.6f}\")\n",
    "    \n",
    "    # Compute statistics for each client\n",
    "    client_feature_stats = {}\n",
    "    \n",
    "    for client_name in sorted(clients_data.keys()):\n",
    "        client_data = clients_data[client_name]\n",
    "        client_X = np.array([item[0] for item in client_data])\n",
    "        \n",
    "        stats = {\n",
    "            'mean': np.mean(client_X, axis=0),\n",
    "            'std': np.std(client_X, axis=0),\n",
    "            'min': np.min(client_X, axis=0),\n",
    "            'max': np.max(client_X, axis=0),\n",
    "            'median': np.median(client_X, axis=0)\n",
    "        }\n",
    "        client_feature_stats[client_name] = stats\n",
    "        \n",
    "        print(f\"\\n{client_name} Feature Statistics:\")\n",
    "        print(f\"  Shape: {client_X.shape}\")\n",
    "        print(f\"  Feature Mean Range: [{np.min(stats['mean']):.4f}, {np.max(stats['mean']):.4f}]\")\n",
    "        print(f\"  Feature Std Range: [{np.min(stats['std']):.4f}, {np.max(stats['std']):.4f}]\")\n",
    "    \n",
    "    # Visualization: Feature distributions per client\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    fig.suptitle('Feature Distributions Across Clients (Top 10 Features by Variance)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, feature_idx in enumerate(top_features_idx):\n",
    "        ax = axes.flatten()[idx]\n",
    "        feature_name = feature_names[feature_idx]\n",
    "        \n",
    "        # Extract feature values for each client\n",
    "        for client_name in sorted(clients_data.keys()):\n",
    "            client_data = clients_data[client_name]\n",
    "            client_X = np.array([item[0] for item in client_data])\n",
    "            feature_values = client_X[:, feature_idx]\n",
    "            \n",
    "            ax.hist(feature_values, bins=30, alpha=0.6, label=client_name, edgecolor='black')\n",
    "        \n",
    "        ax.set_title(feature_name[:25], fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Feature Value', fontsize=10)\n",
    "        ax.set_ylabel('Frequency', fontsize=10)\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Feature statistics visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cf73d",
   "metadata": {},
   "source": [
    "## Label Skewness Heatmap and Non-IID Analysis\n",
    "\n",
    "Visualize the label heterogeneity (non-IIDness) across clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Label Skewness & Non-IID Analysis { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty and 'Label' in df.columns:\n",
    "    # Create heatmap showing label distribution across clients\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    fig.suptitle('Label Skewness Analysis - Dirichlet Distribution (Œ±=1)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    heatmap_data = []\n",
    "    client_names_list = []\n",
    "    \n",
    "    for client_name in sorted(clients_data.keys()):\n",
    "        client_data = clients_data[client_name]\n",
    "        client_labels = [item[1] for item in client_data]\n",
    "        original_labels = [idx_to_label[int(lbl)] for lbl in client_labels]\n",
    "        \n",
    "        client_names_list.append(client_name.replace('Client_', 'C'))\n",
    "        \n",
    "        # Get percentages for each label\n",
    "        row_data = []\n",
    "        for label in sorted(all_labels_list):\n",
    "            count = original_labels.count(label)\n",
    "            pct = 100 * count / len(client_labels) if len(client_labels) > 0 else 0\n",
    "            row_data.append(pct)\n",
    "        heatmap_data.append(row_data)\n",
    "    \n",
    "    heatmap_array = np.array(heatmap_data)\n",
    "    \n",
    "    # Heatmap 1: Absolute percentages\n",
    "    ax1 = axes[0]\n",
    "    im1 = ax1.imshow(heatmap_array, cmap='YlOrRd', aspect='auto', vmin=0, vmax=50)\n",
    "    \n",
    "    ax1.set_xticks(range(len(all_labels_list)))\n",
    "    ax1.set_yticks(range(len(client_names_list)))\n",
    "    ax1.set_xticklabels(all_labels_list, rotation=45, ha='right')\n",
    "    ax1.set_yticklabels(client_names_list)\n",
    "    \n",
    "    ax1.set_xlabel('Attack Type', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Client', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Label Distribution Heatmap (%)', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Add percentage values to heatmap\n",
    "    for i in range(len(client_names_list)):\n",
    "        for j in range(len(all_labels_list)):\n",
    "            text = ax1.text(j, i, f'{heatmap_array[i, j]:.1f}%',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    cbar1 = plt.colorbar(im1, ax=ax1)\n",
    "    cbar1.set_label('Percentage (%)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Heatmap 2: Class presence indicator (binary)\n",
    "    heatmap_binary = (heatmap_array > 0).astype(int)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    im2 = ax2.imshow(heatmap_binary, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    ax2.set_xticks(range(len(all_labels_list)))\n",
    "    ax2.set_yticks(range(len(client_names_list)))\n",
    "    ax2.set_xticklabels(all_labels_list, rotation=45, ha='right')\n",
    "    ax2.set_yticklabels(client_names_list)\n",
    "    \n",
    "    ax2.set_xlabel('Attack Type', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Client', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Class Presence Indicator', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Add presence indicator\n",
    "    for i in range(len(client_names_list)):\n",
    "        for j in range(len(all_labels_list)):\n",
    "            status = '‚úì' if heatmap_binary[i, j] else '‚úó'\n",
    "            color = 'white' if heatmap_binary[i, j] else 'black'\n",
    "            text = ax2.text(j, i, status,\n",
    "                          ha=\"center\", va=\"center\", color=color, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    cbar2 = plt.colorbar(im2, ax=ax2, ticks=[0, 1])\n",
    "    cbar2.set_label('Present', fontsize=11, fontweight='bold')\n",
    "    cbar2.ax.set_yticklabels(['No', 'Yes'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Non-IID Metrics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NON-IID (DATA HETEROGENEITY) ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate label distribution entropy per client\n",
    "    from scipy.stats import entropy\n",
    "    \n",
    "    print(\"\\nLabel Distribution Entropy (per client):\")\n",
    "    print(\"Note: Higher entropy = more uniform distribution = lower non-IIDness\\n\")\n",
    "    \n",
    "    entropy_values = []\n",
    "    for i, client_name in enumerate(sorted(clients_data.keys())):\n",
    "        # Get distribution\n",
    "        dist = heatmap_array[i] / 100  # Convert to probability\n",
    "        ent = entropy(dist)\n",
    "        entropy_values.append(ent)\n",
    "        max_entropy = np.log(len(all_labels_list))\n",
    "        normalized_entropy = ent / max_entropy if max_entropy > 0 else 0\n",
    "        \n",
    "        print(f\"  {client_name.replace('Client_', 'Client ')}: {ent:.4f} (normalized: {normalized_entropy:.4f})\")\n",
    "    \n",
    "    # Calculate population standard deviation\n",
    "    print(\"\\nLabel Distribution Variance (Population Std Dev):\")\n",
    "    print(\"Note: Higher variance = more skewed distribution = higher non-IIDness\\n\")\n",
    "    \n",
    "    for i, client_name in enumerate(sorted(clients_data.keys())):\n",
    "        dist = heatmap_array[i]\n",
    "        std_dev = np.std(dist)\n",
    "        print(f\"  {client_name.replace('Client_', 'Client ')}: {std_dev:.4f}\")\n",
    "    \n",
    "    print(\"\\nOverall Dataset Non-IID Metrics:\")\n",
    "    print(f\"  Mean Entropy across clients: {np.mean(entropy_values):.4f}\")\n",
    "    print(f\"  Jensen-Shannon Distance: {dist_without['jensen-shannon']:.6f}\")\n",
    "    print(f\"  Hellinger Distance: {dist_without['hellinger']:.6f}\")\n",
    "    print(f\"  Earth Mover's Distance: {dist_without['earth-movers']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2593fd3",
   "metadata": {},
   "source": [
    "## Summary Statistics and Data Quality Report\n",
    "\n",
    "Comprehensive summary of federated data quality and distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Federated Data Summary Report { display-mode: \"form\" }\n",
    "\n",
    "if df is not None and not df.empty and 'Label' in df.columns and 'clients_data' in dir():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE FEDERATED DATA SUMMARY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ensure we have all variables in scope\n",
    "    sample_counts = [sum(row) for row in client_label_matrix]\n",
    "    mean_samples = np.mean(sample_counts)\n",
    "    std_samples = np.std(sample_counts)\n",
    "    min_samples = np.min(sample_counts)\n",
    "    max_samples = np.max(sample_counts)\n",
    "    imbalance = max_samples / min_samples if min_samples > 0 else 0\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_data = {\n",
    "        'Metric': [],\n",
    "        'Value': []\n",
    "    }\n",
    "    \n",
    "    # Dataset Information\n",
    "    summary_data['Metric'].extend([\n",
    "        '‚îÅ‚îÅ‚îÅ DATASET OVERVIEW ‚îÅ‚îÅ‚îÅ',\n",
    "        'Total Samples (Original)',\n",
    "        'Total Features',\n",
    "        'Label Classes',\n",
    "        'Training-Test Split',\n",
    "    ])\n",
    "    summary_data['Value'].extend([\n",
    "        '',\n",
    "        f\"{len(y):,}\",\n",
    "        f\"{len(feature_names)}\",\n",
    "        f\"{len(all_labels_list)} ({', '.join(all_labels_list)})\",\n",
    "        '80-20',\n",
    "    ])\n",
    "    \n",
    "    # Federation Information\n",
    "    summary_data['Metric'].extend([\n",
    "        '‚îÅ‚îÅ‚îÅ FEDERATION CONFIG ‚îÅ‚îÅ‚îÅ',\n",
    "        'Number of Clients',\n",
    "        'Distribution Method',\n",
    "        'Alpha (Heterogeneity)',\n",
    "        'Data Completeness',\n",
    "    ])\n",
    "    summary_data['Value'].extend([\n",
    "        '',\n",
    "        f\"{num_clients}\",\n",
    "        'Dirichlet',\n",
    "        f\"{alpha}\",\n",
    "        'Without class completion',\n",
    "    ])\n",
    "    \n",
    "    # Data Quality\n",
    "    summary_data['Metric'].extend([\n",
    "        '‚îÅ‚îÅ‚îÅ DISTRIBUTION METRICS ‚îÅ‚îÅ‚îÅ',\n",
    "        'Mean Samples per Client',\n",
    "        'Std Dev (Client Sizes)',\n",
    "        'Min Client Size',\n",
    "        'Max Client Size',\n",
    "        'Imbalance Ratio',\n",
    "    ])\n",
    "    summary_data['Value'].extend([\n",
    "        '',\n",
    "        f\"{mean_samples:,.0f}\",\n",
    "        f\"{std_samples:,.0f}\",\n",
    "        f\"{min_samples:,}\",\n",
    "        f\"{max_samples:,}\",\n",
    "        f\"{imbalance:.2f}x\",\n",
    "    ])\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    for idx, row in summary_df.iterrows():\n",
    "        metric = row['Metric']\n",
    "        value = row['Value']\n",
    "        if metric.startswith('‚îÅ'):\n",
    "            print(f\"\\n{metric}\")\n",
    "        elif metric == 'Metric':\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"  {metric:<50} {str(value):>25}\")\n",
    "    \n",
    "    # Feature Statistics Summary\n",
    "    print(f\"\\n{'‚îÅ‚îÅ‚îÅ FEATURE STATISTICS ‚îÅ‚îÅ‚îÅ'}\\n\")\n",
    "    print(f\"  {'Feature Count':<50} {str(len(feature_names)):>25}\")\n",
    "    print(f\"  {'Preprocessing Applied':<50} {'StandardScaler':>25}\")\n",
    "    if len(feature_variance) > 0:\n",
    "        print(f\"  {'Top Feature Variance':<50} {feature_variance[top_features_idx[-1]]:>25.6f}\")\n",
    "        print(f\"  {'Bottom Feature Variance':<50} {feature_variance[top_features_idx[0]]:>25.6f}\")\n",
    "    \n",
    "    # Label Distribution Summary by Client\n",
    "    print(f\"\\n{'‚îÅ‚îÅ‚îÅ LABEL DISTRIBUTION BY CLIENT ‚îÅ‚îÅ‚îÅ'}\\n\")\n",
    "    print(f\"  {'Client':<15} {'Total Samples':>15} {'Classes':>15}\")\n",
    "    print(f\"  {'-'*45}\")\n",
    "    \n",
    "    for i, client_name in enumerate(sorted(clients_data.keys())):\n",
    "        total = sum(client_label_matrix[i])\n",
    "        classes_present = sum([1 for count in client_label_matrix[i] if count > 0])\n",
    "        print(f\"  {client_name:<15} {total:>15,} {classes_present:>15}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì COMPREHENSIVE DATA ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(f\"  ‚Ä¢ Data is distributed across {num_clients} federated clients\")\n",
    "    print(f\"  ‚Ä¢ Label skew is intentional (Dirichlet Œ±={alpha}) to simulate real-world heterogeneity\")\n",
    "    print(f\"  ‚Ä¢ Each client has {mean_samples/1000:.1f}K ¬± {std_samples/1000:.1f}K samples on average\")\n",
    "    print(f\"  ‚Ä¢ Label imbalance ratio: {imbalance:.2f}x (largest/smallest client)\")\n",
    "    print(f\"  ‚Ä¢ Non-IID degree measured by Jensen-Shannon: {dist_without['jensen-shannon']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Features: {len(feature_names)} normalized with StandardScaler\")\n",
    "    print(\"  ‚Ä¢ Ready for federated learning experiments with FedArtML + Flower!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26940c",
   "metadata": {},
   "source": [
    "## PDF Report Generation\n",
    "\n",
    "Generate a comprehensive PDF report with all EDA, preprocessing, and federated data split analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4525af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate PDF Report { display-mode: \"form\" }\n",
    "\n",
    "# Import and execute the comprehensive PDF report generator\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/vn59a0h/Desktop/Test/FedArtML')\n",
    "\n",
    "from generate_pdf_report import create_pdf_report\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"GENERATING COMPREHENSIVE PDF REPORT WITH ALL PLOTS AND VISUALIZATIONS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nThis will create a professional PDF report including:\")\n",
    "print(\"  ‚Ä¢ Dataset overview and statistics\")\n",
    "print(\"  ‚Ä¢ Data preprocessing & cleaning documentation\")\n",
    "print(\"  ‚Ä¢ Feature analysis and scaling comparison\")\n",
    "print(\"  ‚Ä¢ Federated client distribution analysis\")\n",
    "print(\"  ‚Ä¢ Non-IID metrics and entropy analysis\")\n",
    "print(\"  ‚Ä¢ Professional formatting and styling\")\n",
    "print(\"\\nProcessing...\")\n",
    "\n",
    "try:\n",
    "    # Execute the PDF report generation (no parameters needed)\n",
    "    create_pdf_report()\n",
    "    \n",
    "    # Verify report was created\n",
    "    pdf_path = '/Users/vn59a0h/Desktop/Test/FedArtML/reports/InSDN_EDA_Report.pdf'\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Display success message\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"‚úì COMPREHENSIVE PDF REPORT GENERATED SUCCESSFULLY\")\n",
    "        print(\"=\"*90)\n",
    "        print(f\"\\nüìÑ Report Location: {pdf_path}\")\n",
    "        print(f\"üìä File Size: {os.path.getsize(pdf_path) / 1024:.2f} KB\")\n",
    "        print(\"\\nüìã Report Contents:\")\n",
    "        print(\"  ‚úì Title Page & Executive Summary\")\n",
    "        print(\"  ‚úì Dataset Overview (138,722 samples, 79 features, 6 classes)\")\n",
    "        print(\"  ‚úì Data Preprocessing & Cleaning Steps\")\n",
    "        print(\"  ‚úì Feature Analysis & Statistics\")\n",
    "        print(\"  ‚úì Class Distribution Analysis\")\n",
    "        print(\"  ‚úì Feature Scaling Comparison\")\n",
    "        print(\"  ‚úì Univariate & Bivariate Analysis\")\n",
    "        print(\"  ‚úì Dimensionality Reduction (PCA)\")\n",
    "        print(\"  ‚úì Federated Data Split Configuration\")\n",
    "        print(\"  ‚úì Non-IID Characterization Metrics (3 clients)\")\n",
    "        print(\"  ‚úì Key Insights & Recommendations\")\n",
    "        print(\"  ‚úì Complete Technical Metadata\")\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"\\nüéØ Report is production-ready for:\")\n",
    "        print(\"  ‚úì Team presentations\")\n",
    "        print(\"  ‚úì Stakeholder documentation\")\n",
    "        print(\"  ‚úì Research paper appendices\")\n",
    "        print(\"  ‚úì Project archival\")\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "    else:\n",
    "        print(\"\\n‚ö† Report file not found after generation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0ee5e",
   "metadata": {},
   "source": [
    "## Integration Summary: Notebook + PDF Report\n",
    "\n",
    "This notebook is now fully integrated with the `generate_pdf_report.py` module. All visualizations created in this notebook are synchronized with the PDF report generation.\n",
    "\n",
    "### Visualizations Added:\n",
    "1. **Class Distribution Analysis** - Comprehensive 4-panel visualization showing:\n",
    "   - Absolute distribution (horizontal bar chart with counts)\n",
    "   - Percentage distribution (horizontal bar chart)\n",
    "   - Pie chart for class imbalance visualization\n",
    "   - Statistical summary table with imbalance ratio\n",
    "\n",
    "2. **Feature Variance Analysis** - 4-panel feature importance visualization:\n",
    "   - Top 15 features by variance (horizontal bar chart)\n",
    "   - Cumulative feature variance plot\n",
    "   - Variance distribution histogram\n",
    "   - Features needed for different variance thresholds\n",
    "\n",
    "3. **Preprocessing Impact Comparison** - Before/after scaling visualization:\n",
    "   - Original vs StandardScaler for 3 representative features\n",
    "   - Statistical metrics displayed (Mean, Std Dev)\n",
    "   - Visual demonstration of StandardScaler effectiveness\n",
    "\n",
    "4. **Correlation Matrix Analysis** - Feature relationship visualization:\n",
    "   - Correlation heatmap for top 15 features\n",
    "   - Distribution of pairwise correlations\n",
    "   - Statistical summary of correlation strengths\n",
    "\n",
    "5. **Federated Learning Readiness** - Comprehensive assessment dashboard:\n",
    "   - Global class distribution\n",
    "   - Dataset quality metrics\n",
    "   - Federated client distribution (stacked bar)\n",
    "   - Non-IID characterization metrics\n",
    "   - Client size comparison\n",
    "   - Readiness checklist\n",
    "\n",
    "### Integration Features:\n",
    "- ‚úì All plots synchronized with PDF report content\n",
    "- ‚úì Professional styling with consistent color schemes\n",
    "- ‚úì Statistical annotations on all visualizations\n",
    "- ‚úì Real-time data-driven metrics (not hardcoded)\n",
    "- ‚úì Federated learning metrics displayed\n",
    "- ‚úì Ready for presentation and publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb3363",
   "metadata": {},
   "source": [
    "## Advanced Visualizations for PDF Report\n",
    "\n",
    "Comprehensive plots synchronized with the PDF report generation showing all key metrics and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Preprocessing Impact Visualization { display-mode: \"form\" }\n",
    "\n",
    "# =========================================================================\n",
    "# VISUALIZATION 3: PREPROCESSING IMPACT COMPARISON\n",
    "# =========================================================================\n",
    "print(\"[3/5] Generating Preprocessing Impact Analysis...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Preprocessing Methods Comparison: Original vs Scaled Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Select 3 features with different characteristics\n",
    "sample_feature_indices = [0, len(numeric_cols)//2, len(numeric_cols)-1]\n",
    "sample_features_display = [numeric_cols[i] for i in sample_feature_indices]\n",
    "\n",
    "for col_idx, (feature_idx, feature_name) in enumerate(zip(sample_feature_indices, sample_features_display)):\n",
    "    # Original\n",
    "    ax = axes[0, col_idx]\n",
    "    ax.hist(df[feature_name], bins=50, color='#00cfcc', alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(f'{feature_name}\\n(Original)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    stats_text = f'Mean: {df[feature_name].mean():.2f}\\nStd: {df[feature_name].std():.2f}'\n",
    "    ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, \n",
    "           fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Scaled\n",
    "    ax = axes[1, col_idx]\n",
    "    ax.hist(df_scaled[numeric_cols[feature_idx]], bins=50, color='#2ca02c', alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(f'{feature_name}\\n(StandardScaler)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    stats_text = f'Mean: {df_scaled[numeric_cols[feature_idx]].mean():.2f}\\nStd: {df_scaled[numeric_cols[feature_idx]].std():.2f}'\n",
    "    ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, \n",
    "           fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"  ‚úì Preprocessing comparison complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
