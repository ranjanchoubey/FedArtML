{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1352b66",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ranjanchoubey/FedArtML/blob/main/examples/13_Get_started_create_FL_data_label_skew_dirichlet_InSDN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db1bb9",
   "metadata": {},
   "source": [
    "# Getting started with FedArtML creating FL data (label skew) and implementing an FL model\n",
    "\n",
    "Our ***FedArTML*** tool facilitates the generation of non-IID datasets\n",
    "in a controlled way to support federated learning (FL) research for federated datasets from centralized datasets. It includes classes and functions to create federated datasets from centralized data, given the **number** of **clients** and the degree of heterogeneity (**non-IID-ness**) desired.\n",
    "\n",
    "This guide aims to **understand** using the **SplitAsFederatedData** class to create federated datasets with the **Dirichlet method** for **Label skew**. We use the `InSDN` dataset for our tests. Moreover, we include the `Flower framework` to train an FL model using the output from the FedArtML library.\n",
    "\n",
    "**Notes:**\n",
    "1. To check the source code, you can visit the [GitHub repo](https://github.com/Sapienza-University-Rome/FedArtML)\n",
    "2. To check the documentation, you visit the [dedicated link](https://fedartml.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b6641",
   "metadata": {},
   "source": [
    "# Install libraries\n",
    "\n",
    "First, installing the FedArtML and Flower libraries from Pypi (the latest version) is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install FedArtML Library { display-mode: \"form\" }\n",
    "!pip install -q fedartml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c833f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install Flower Framework { display-mode: \"form\" }\n",
    "!pip install -q flwr[simulation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6b91c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Libraries & Configure Environment { display-mode: \"form\" }\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", rc = {'figure.figsize':(5,7)})\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Garbage Collector - use it like gc.collect()\n",
    "import gc\n",
    "\n",
    "# Custom Callback To Include in Callbacks List At Training Time\n",
    "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "\n",
    "from fedartml import InteractivePlots, SplitAsFederatedData\n",
    "\n",
    "# Make TensorFlow logs less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import flwr as fl\n",
    "\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from flwr.common import Metrics\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd1f140",
   "metadata": {},
   "source": [
    "# Define parameters\n",
    "\n",
    "Then it is necessary to define some parameters to run this notebook smoothly. The most important are:\n",
    "\n",
    "\n",
    "\n",
    "*   `local_nodes_glob`: Defines the desired number of clients (local nodes) the centralized data will be divided into. In this case, we set it as 3.\n",
    "*   `Alpha`: Using the \"Dirichlet-based\" method, we need to define the `Alpha`, which will determine the degree of heterogeneity of the client's distribution (non-IID-ness). Notice that the smaller the value of `Alpha`, the higher the non-IID-ness. In this case, we set it as 1. Those parameters can be changed if desired to get familiar with their use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random state for reproducibility\n",
    "random_state = 0  # @param {type: \"integer\", min: 0, max: 10000, step: 1}\n",
    "\n",
    "# Define colors to use in plots\n",
    "colors = [\"#00cfcc\",\"#e6013b\",\"#007f88\",\"#00cccd\",\"#69e0da\",\"darkblue\",\"#FFFFFF\"]\n",
    "\n",
    "# Path to InSDN dataset\n",
    "dataset_path = '../data/LINK_all_features_all_datsets/InSDN'\n",
    "\n",
    "print(\"\\n✓ Initialization complete. Load data and then configure federated learning parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07409718",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "For the sake of order, we defined some functions that will be used in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define Helper Functions { display-mode: \"form\" }\n",
    "\n",
    "# Define function to test a model and retrieve classification metrics\n",
    "def test_model(model, X_test, Y_test):\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n",
    "    logits = model.predict(X_test, batch_size=32, verbose=1, callbacks=[GarbageCollectorCallback()])\n",
    "    y_pred = tf.argmax(logits, axis=1)\n",
    "    loss = cce(Y_test, logits).numpy()\n",
    "    acc = accuracy_score(y_pred, Y_test)\n",
    "    pre = precision_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
    "    rec = recall_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
    "    f1s = f1_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
    "\n",
    "    return loss, acc, pre, rec, f1s\n",
    "\n",
    "# Define function to convert from SplitAsFederatedData function output (FedArtML) to Flower (list) format\n",
    "def from_FedArtML_to_Flower_format(clients_dict):\n",
    "  # initialize list that contains clients (features and labels) to extract later from client_fn in Flower\n",
    "  list_x_train = []\n",
    "  list_y_train = []\n",
    "\n",
    "  # Get the name of the clients from the dictionary\n",
    "  client_names = list(clients_dict.keys())\n",
    "\n",
    "  # Iterate over each client\n",
    "  for client in client_names:\n",
    "    # Get data from each client\n",
    "    each_client_train=np.array(clients_dict[client],dtype=object)\n",
    "\n",
    "    # Extract features for each client\n",
    "    feat=[]\n",
    "    x_tra=np.array(each_client_train[:, 0])\n",
    "    for row in x_tra:\n",
    "      feat.append(row)\n",
    "    feat=np.array(feat)\n",
    "\n",
    "    # Extract labels from each client\n",
    "    y_tra=np.array(each_client_train[:, 1])\n",
    "\n",
    "    # Append in list features and labels to extract later from client_fn in Flower\n",
    "    list_x_train.append(feat)\n",
    "    list_y_train.append(y_tra)\n",
    "\n",
    "  return list_x_train, list_y_train\n",
    "\n",
    "# Define Dense Neural Network (DNN) model for tabular InSDN data\n",
    "def DNN_model(input_shape, num_classes):\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Input(shape=(input_shape,)))\n",
    "  model.add(layers.Dense(128, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Dropout(0.3))\n",
    "  model.add(layers.Dense(64, activation='relu'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Dropout(0.3))\n",
    "  model.add(layers.Dense(32, activation='relu'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "# Define local training/evaluation function\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, x_train, y_train, x_test, y_test, epochs_client) -> None:\n",
    "        self.model = model\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.epochs_client = epochs_client\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.x_train, self.y_train, validation_split=0.1, epochs=self.epochs_client, verbose=2)\n",
    "        return self.model.get_weights(), len(self.x_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, acc = self.model.evaluate(self.x_test, self.y_test, verbose=2)\n",
    "        return loss, len(self.x_test), {\"accuracy\": acc}\n",
    "\n",
    "def plot_metric_from_history(\n",
    "    hist: None,\n",
    "    save_plot_path: None,\n",
    "    metric_type: None,\n",
    "    metric: None,\n",
    ") -> None:\n",
    "    \"\"\"Function to plot from Flower server History.\n",
    "    Parameters\n",
    "    ----------\n",
    "    hist : History\n",
    "        Object containing evaluation for all rounds.\n",
    "    save_plot_path : Path\n",
    "        Folder to save the plot to.\n",
    "    metric_type : Literal[\"centralized\", \"distributed\"]\n",
    "        Type of metric to plot.\n",
    "    metric : Literal[\"accuracy\",\"precision\",\"recall\",\"f1score\"]\n",
    "        Metric to plot.\n",
    "    \"\"\"\n",
    "    metric_dict = (\n",
    "        hist.metrics_centralized\n",
    "        if metric_type == \"centralized\"\n",
    "        else hist.metrics_distributed\n",
    "    )\n",
    "    rounds, values = zip(*metric_dict[metric])\n",
    "    # plt.plot(np.asarray(rounds), np.asarray(values), label=\"FedAvg\")\n",
    "    plt.plot(np.asarray(rounds), np.asarray(values), color=colors[5], linewidth=5, label='Test')\n",
    "    plt.legend(fontsize=45)\n",
    "    plt.xlabel('Communication round', fontsize=40)\n",
    "    plt.ylabel(metric, fontsize=50)\n",
    "    plt.title(metric, fontsize=60)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    # plt.ylim(min(min(min(commun_metrics))) - 0.05, max(max(max(commun_metrics))) + 0.05)\n",
    "    plt.ylim(0, 1)\n",
    "    # plt.savefig(Path(save_plot_path) / Path(f\"{metric_type}_metrics{suffix}.png\"))\n",
    "    # plt.close()\n",
    "\n",
    "def retrieve_global_metrics(\n",
    "    hist: None,\n",
    "    metric_type: None,\n",
    "    metric: None,\n",
    "    best_metric: None,\n",
    ") -> None:\n",
    "    \"\"\"Function to retrieve metrics from Flower server History.\n",
    "    Parameters\n",
    "    ----------\n",
    "    hist : History\n",
    "        Object containing evaluation for all rounds.\n",
    "    metric_type : Literal[\"centralized\", \"distributed\"]\n",
    "        Type of metric to retrieve.\n",
    "    metric : Literal[\"accuracy\",\"precision\",\"recall\",\"f1score\"]\n",
    "        Metric to retrieve.\n",
    "    \"\"\"\n",
    "    metric_dict = (\n",
    "        hist.metrics_centralized\n",
    "        if metric_type == \"centralized\"\n",
    "        else hist.metrics_distributed\n",
    "    )\n",
    "    rounds, values = zip(*metric_dict[metric])\n",
    "    if best_metric:\n",
    "      metric_return = max(values)\n",
    "    else:\n",
    "      metric_return = values[-1]\n",
    "    return metric_return\n",
    "\n",
    "print(\"✓ All helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51599bbd",
   "metadata": {},
   "source": [
    "# Load and preprocess InSDN data\n",
    "\n",
    "The InSDN dataset consists of network traffic data with various features. We will load, preprocess, and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load InSDN Dataset from Official Source { display-mode: \"form\" }\n",
    "\n",
    "# Load InSDN dataset from official UCD source\n",
    "# InSDN: SDN Intrusion Detection Dataset\n",
    "# Source: https://aseados.ucd.ie/datasets/SDN/\n",
    "# Paper: \"InSDN: SDN Intrusion Dataset\" IEEE Access Vol.8, pp 165263-165284, Sep 2020\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "print(\"Loading InSDN dataset...\")\n",
    "print(\"Dataset Information:\")\n",
    "print(\"  - Total instances: 343,939 network flow records\")\n",
    "print(\"  - Normal traffic: 68,424 (20%)\")\n",
    "print(\"  - Attack traffic: 275,515 (80%)\")\n",
    "print(\"  - Features: 80+ network flow characteristics\")\n",
    "print(\"  - Attack types: DoS, DDoS, Probe, Brute Force, Exploitation, Web Attack, Botnet\")\n",
    "\n",
    "df = None\n",
    "\n",
    "# Option 1: Try to load from local path first\n",
    "try:\n",
    "    if os.path.exists(dataset_path):\n",
    "        data_files = [f for f in os.listdir(dataset_path) if f.endswith('.csv')]\n",
    "        if data_files:\n",
    "            print(f\"\\n✓ Found local dataset: {data_files[0]}\")\n",
    "            df = pd.read_csv(os.path.join(dataset_path, data_files[0]))\n",
    "except Exception as e:\n",
    "    print(f\"Local path check failed: {e}\")\n",
    "\n",
    "# Option 2: Download from official UCD source\n",
    "if df is None:\n",
    "    try:\n",
    "        print(\"\\n✓ Attempting to download official InSDN dataset from UCD...\")\n",
    "        zip_url = \"https://aseados.ucd.ie/datasets/SDN/InSDN_DatasetCSV.zip\"\n",
    "        \n",
    "        response = requests.get(zip_url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            print(\"  Download successful! Extracting dataset...\")\n",
    "            \n",
    "            # Extract and load the zip file\n",
    "            zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "            csv_files = [f for f in zip_file.namelist() if f.endswith('.csv')]\n",
    "            \n",
    "            if csv_files:\n",
    "                # Prefer OVS.csv for more attack variety\n",
    "                selected_file = next((f for f in csv_files if 'OVS.csv' in f), csv_files[0])\n",
    "                print(f\"  Loading: {selected_file}\")\n",
    "                df = pd.read_csv(zip_file.open(selected_file))\n",
    "                print(f\"✓ Successfully loaded InSDN dataset! Shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "\n",
    "# If download fails, provide instructions\n",
    "if df is None or df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Could not load dataset automatically\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nPlease download manually from:\")\n",
    "    print(\"  https://aseados.ucd.ie/datasets/SDN/\")\n",
    "    print(\"\\nSteps:\")\n",
    "    print(\"  1. Download: InSDN_DatasetCSV.zip\")\n",
    "    print(\"  2. Extract the ZIP file\")\n",
    "    print(\"  3. Choose one CSV file:\")\n",
    "    print(\"     - OVS.csv (136,743 records - recommended for variety)\")\n",
    "    print(\"     - metasploitable-2.csv (138,772 records)\")\n",
    "    print(\"     - Normal_data.csv (68,424 records - benign traffic only)\")\n",
    "    print(f\"  4. Place in: {os.path.abspath(dataset_path)}\")\n",
    "    print(\"=\"*80)\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Display dataset information\n",
    "if not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"InSDN Dataset Loaded Successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Shape: {df.shape[0]} records x {df.shape[1]} features\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nColumn names ({len(df.columns)} total):\")\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Preprocess InSDN Data { display-mode: \"form\" }\n",
    "\n",
    "# Preprocess InSDN data\n",
    "if not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Preprocessing InSDN Data\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"Column names (first 10): {list(df.columns[:10])}\")\n",
    "    \n",
    "    # The InSDN dataset has 'Label' column (uppercase L)\n",
    "    if 'Label' in df.columns:\n",
    "        y_raw = df['Label'].values\n",
    "        print(f\"\\n✓ Found 'Label' column\")\n",
    "        print(f\"  Unique labels: {np.unique(y_raw)}\")\n",
    "        print(f\"  Label counts: {pd.Series(y_raw).value_counts().to_dict()}\")\n",
    "        \n",
    "        # Drop the Label column and non-numeric metadata columns\n",
    "        X_df = df.drop(columns=['Label', 'Flow ID', 'Src IP', 'Dst IP', 'Timestamp'], errors='ignore')\n",
    "    else:\n",
    "        print(\"ERROR: 'Label' column not found!\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        raise ValueError(\"'Label' column not found in dataset\")\n",
    "    \n",
    "    # Select only numeric columns for features\n",
    "    X_df = X_df.select_dtypes(include=[np.number])\n",
    "    print(f\"\\nNumeric features shape: {X_df.shape}\")\n",
    "    print(f\"Feature columns: {list(X_df.columns[:10])}... (showing first 10)\")\n",
    "    \n",
    "    X = X_df.values\n",
    "    \n",
    "    # Convert labels to numeric (they are strings like 'BFA', 'DoS', etc.)\n",
    "    unique_labels = sorted(set(y_raw))\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y = np.array([label_mapping[label] for label in y_raw]).astype(int)\n",
    "    \n",
    "    print(f\"\\nLabel mapping ({len(label_mapping)} classes):\")\n",
    "    for label, idx in label_mapping.items():\n",
    "        print(f\"  {label} -> {idx}\")\n",
    "    \n",
    "    print(f\"\\nLabel information:\")\n",
    "    print(f\"  - Unique numeric labels: {sorted(np.unique(y))}\")\n",
    "    print(f\"  - Label range: [{y.min()}, {y.max()}]\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    print(f\"\\nOriginal class distribution:\")\n",
    "    for cls, count in zip(unique_classes, class_counts):\n",
    "        original_label = [k for k, v in label_mapping.items() if v == cls][0]\n",
    "        print(f\"  Class {cls} ({original_label}): {count} samples ({100*count/len(y):.1f}%)\")\n",
    "    \n",
    "    # Handle missing values in features\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "    print(\"\\n✓ Missing values handled (mean imputation)\")\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    print(\"✓ Features normalized (StandardScaler)\")\n",
    "    \n",
    "    # Remove classes with very few samples (< 2) to avoid stratification issues\n",
    "    min_samples_per_class = 2\n",
    "    valid_classes = unique_classes[class_counts >= min_samples_per_class]\n",
    "    mask = np.isin(y, valid_classes)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"\\nFiltered class distribution (removed classes with < {min_samples_per_class} samples):\")\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    for cls, count in zip(unique_classes, class_counts):\n",
    "        original_label = [k for k, v in label_mapping.items() if v == cls][0]\n",
    "        print(f\"  Class {cls} ({original_label}): {count} samples ({100*count/len(y):.1f}%)\")\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    # Use stratified split if possible, otherwise use random split\n",
    "    try:\n",
    "        x_train_glob, x_test_glob, y_train_glob, y_test_glob = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "        )\n",
    "        print(\"\\n✓ Used stratified train-test split\")\n",
    "    except ValueError:\n",
    "        # If stratification fails, use random split\n",
    "        x_train_glob, x_test_glob, y_train_glob, y_test_glob = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state, stratify=None\n",
    "        )\n",
    "        print(\"✓ Used random train-test split (stratification not possible)\")\n",
    "    \n",
    "    # Get dataset statistics\n",
    "    num_classes = len(np.unique(y_train_glob))\n",
    "    input_shape = x_train_glob.shape[1]\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  - Training set: {x_train_glob.shape}\")\n",
    "    print(f\"  - Test set: {x_test_glob.shape}\")\n",
    "    print(f\"  - Number of classes: {num_classes}\")\n",
    "    print(f\"  - Input shape: {input_shape}\")\n",
    "    print(f\"  - Label range (train): [{y_train_glob.min()}, {y_train_glob.max()}]\")\n",
    "    print(f\"  - Label range (test): [{y_test_glob.min()}, {y_test_glob.max()}]\")\n",
    "    print(f\"\\nClass distribution in training set:\")\n",
    "    unique, counts = np.unique(y_train_glob, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        pct = 100 * count / len(y_train_glob)\n",
    "        original_label = [k for k, v in label_mapping.items() if v == cls][0]\n",
    "        print(f\"    Class {cls} ({original_label}): {count} samples ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"Cannot preprocess - dataset is empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualize Class Distributions { display-mode: \"form\" }\n",
    "\n",
    "# Visualize class distributions\n",
    "if not df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training set\n",
    "    unique_train, counts_train = np.unique(y_train_glob, return_counts=True)\n",
    "    axes[0].bar(unique_train, counts_train, color=colors[0], alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Label', fontsize=12)\n",
    "    axes[0].set_ylabel('Count', fontsize=12)\n",
    "    axes[0].set_title('Centralized Datatset : Training Set Label Distribution', fontsize=14)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Test set\n",
    "    unique_test, counts_test = np.unique(y_test_glob, return_counts=True)\n",
    "    axes[1].bar(unique_test, counts_test, color=colors[1], alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Label', fontsize=12)\n",
    "    axes[1].set_ylabel('Count', fontsize=12)\n",
    "    axes[1].set_title('Centralized Datatset : Test Set Label Distribution', fontsize=14)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c129b8",
   "metadata": {},
   "source": [
    "# Create the federated dataset\n",
    "\n",
    "This core function allows the creation of FL data from centralized data. The class `SplitAsFederatedData` is instantiated using a `random_state` to reproduce the results.\n",
    "\n",
    "Then, the `.create_clients` function performs the federation of the centralized data by taking the features and labels, defining the number of desired clients and setting the Alpha value.\n",
    "\n",
    "**Note:** When creating federated data and setting heterogeneous distributions (i.e. high values of percent_noniid or small values of Alpha), it is more likely the clients hold examples from only one class. Then, two cases are returned as output for fed_data and distances:\n",
    "- \"with_class_completion\": In this case, the clients are completed with one (random) example of each missing class for each client to have all the label's classes.\n",
    "- \"without_class_completion\": In this case, the clients are NOT completed with one (random) example of each missing class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff58c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title FEDERATED LEARNING CONFIGURATION: Set Parameters Here { display-mode: \"form\" }\n",
    "\n",
    "# ================================================================================\n",
    "# USER INPUT - SET THESE PARAMETERS ONLY ONCE\n",
    "# These will be used throughout the entire notebook for federated dataset creation\n",
    "# ================================================================================\n",
    "\n",
    "# Number of clients/local nodes for federated learning\n",
    "num_clients = 5  # @param {type: \"integer\", min: 2, max: 50, step: 1}\n",
    "\n",
    "# Distribution method for data heterogeneity\n",
    "distribution_method = \"dirichlet\"  # @param [\"dirichlet\", \"percent_noniid\", \"no-label-skew\"]\n",
    "\n",
    "# Heterogeneity parameter (alpha)\n",
    "# - For Dirichlet: controls concentration (smaller = more non-IID)\n",
    "# - For Percent Non-IID: converted to percentage (0-100)\n",
    "# - For IID: ignored (uniform distribution)\n",
    "alpha = 1  # @param {type: \"number\", min: 0.001, max: 10.0, step: 0.1}\n",
    "\n",
    "# ================================================================================\n",
    "# DISPLAY CONFIGURATION SUMMARY\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEDERATED LEARNING CONFIGURATION - Ready to Create Dataset\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n  ✓ Number of Clients: {num_clients}\")\n",
    "print(f\"  ✓ Distribution Method: {distribution_method.upper()}\")\n",
    "print(f\"  ✓ Heterogeneity Parameter (alpha): {alpha}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SELECTED METHOD DETAILS:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if distribution_method == \"dirichlet\":\n",
    "    print(f\"\\n  METHOD: Dirichlet Distribution (LABEL SKEW)\")\n",
    "    print(f\"  Alpha = {alpha}\")\n",
    "    print(f\"\\n  What it does:\")\n",
    "    print(f\"    • Each client receives a different subset of labels\")\n",
    "    print(f\"    • Smaller alpha → More non-IID (heterogeneous)\")\n",
    "    print(f\"    • Larger alpha → More balanced (approaching IID)\")\n",
    "\n",
    "elif distribution_method == \"percent_noniid\":\n",
    "    percent_val = max(0, min(100, (alpha / 10.0) * 100))\n",
    "    print(f\"\\n  METHOD: Percent Non-IID Distribution\")\n",
    "    print(f\"  Alpha = {alpha} (converted to {percent_val:.1f}% non-IID)\")\n",
    "    print(f\"\\n  What it does:\")\n",
    "    print(f\"    • Each client has different class distributions\")\n",
    "    print(f\"    • Based on percentage of non-IID samples\")\n",
    "\n",
    "elif distribution_method == \"no-label-skew\":\n",
    "    print(f\"\\n  METHOD: No Label Skew (IID Distribution)\")\n",
    "    print(f\"  Alpha parameter: IGNORED\")\n",
    "    print(f\"\\n  What it does:\")\n",
    "    print(f\"    • Each client gets balanced class distribution\")\n",
    "    print(f\"    • All clients have similar label proportions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Ready to create {num_clients} clients with {distribution_method} method\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Instantiate a SplitAsFederatedData object\n",
    "    my_federater = SplitAsFederatedData(random_state = random_state)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING FEDERATED DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Method: {distribution_method}\")\n",
    "    print(f\"Number of clients: {num_clients}\")\n",
    "    \n",
    "    # Create clients based on selected distribution method\n",
    "    if distribution_method == \"dirichlet\":\n",
    "        # Dirichlet: use alpha directly (0.001-10.0)\n",
    "        print(f\"Alpha: {alpha}\")\n",
    "        clients_glob_dic, list_ids_sampled_dic, miss_class_per_node, distances = my_federater.create_clients(\n",
    "            image_list = x_train_glob, \n",
    "            label_list = y_train_glob,\n",
    "            num_clients = num_clients,  # USE num_clients FROM USER INPUT\n",
    "            prefix_cli='Local_node',\n",
    "            method = \"dirichlet\",\n",
    "            alpha = alpha  # USE alpha FROM USER INPUT\n",
    "        )\n",
    "        print(f\"\\n✓ Dirichlet distribution created successfully\")\n",
    "        \n",
    "    elif distribution_method == \"percent_noniid\":\n",
    "        # Percent Non-IID: convert alpha to percentage (0-100)\n",
    "        percent_value = max(0, min(100, (alpha / 10.0) * 100))\n",
    "        print(f\"Alpha: {alpha} → {percent_value:.1f}% non-IID\")\n",
    "        clients_glob_dic, list_ids_sampled_dic, miss_class_per_node, distances = my_federater.create_clients(\n",
    "            image_list = x_train_glob, \n",
    "            label_list = y_train_glob,\n",
    "            num_clients = num_clients,  # USE num_clients FROM USER INPUT\n",
    "            prefix_cli='Local_node',\n",
    "            method = \"percent_noniid\",\n",
    "            alpha = percent_value  # USE converted alpha FROM USER INPUT\n",
    "        )\n",
    "        print(f\"\\n✓ Percent Non-IID distribution created successfully\")\n",
    "        \n",
    "    elif distribution_method == \"no-label-skew\":\n",
    "        # IID: ignore alpha (uniform distribution)\n",
    "        print(f\"Alpha: IGNORED (IID distribution)\")\n",
    "        clients_glob_dic, list_ids_sampled_dic, miss_class_per_node, distances = my_federater.create_clients(\n",
    "            image_list = x_train_glob, \n",
    "            label_list = y_train_glob,\n",
    "            num_clients = num_clients,  # USE num_clients FROM USER INPUT\n",
    "            prefix_cli='Local_node',\n",
    "            method = \"no-label-skew\",\n",
    "            alpha = None\n",
    "        )\n",
    "        print(f\"\\n✓ IID (no label skew) distribution created successfully\")\n",
    "\n",
    "    clients_glob = clients_glob_dic['without_class_completion']\n",
    "    list_ids_sampled = list_ids_sampled_dic['without_class_completion']\n",
    "\n",
    "    # Convert from SplitAsFederatedData function output (FedArtML) to Flower (list) format\n",
    "    list_x_train, list_y_train = from_FedArtML_to_Flower_format(clients_dict=clients_glob)\n",
    "    \n",
    "    print(f\"\\nFederated dataset statistics:\")\n",
    "    print(f\"  - Number of clients created: {len(list_x_train)}\")\n",
    "    print(f\"  - Training samples per client: {[len(x) for x in list_x_train]}\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Calculate distances\n",
    "    JSD_glob = distances['without_class_completion']['jensen-shannon']\n",
    "    print(\"Jensen-Shannon distance:\", JSD_glob)\n",
    "    HD_glob = distances['without_class_completion']['hellinger']\n",
    "    print(\"Hellinger distance:\", HD_glob)\n",
    "    EMD_glob = distances['without_class_completion']['earth-movers']\n",
    "    print(\"Earth Mover's distance:\", EMD_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # ================================================================================\n",
    "    # STANDARD COLOR PALETTE VISUALIZATION - Professional & Accessible\n",
    "    # ================================================================================\n",
    "    \n",
    "    # Use standard professional color palettes from seaborn/matplotlib\n",
    "    # These are tested, accessible, and widely used in scientific visualization\n",
    "    standard_palette = sns.color_palette(\"husl\", len(label_mapping))\n",
    "    \n",
    "    # Create mapping of labels to standard colors\n",
    "    label_to_standard_color = {\n",
    "        label: standard_palette[idx] for idx, label in enumerate(sorted(label_mapping.keys()))\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"CLIENT DISTRIBUTION VISUALIZATION - STANDARD COLOR PALETTE\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Create figure for standard palette comparison\n",
    "    fig = plt.figure(figsize=(24, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    standard_plot_data = {}\n",
    "    \n",
    "    for key, value in clients_glob.items():\n",
    "        labels_check = []\n",
    "        for i in range(len(value)):\n",
    "            val = value[i][1]\n",
    "            labels_check.append(val)\n",
    "        \n",
    "        labels_check = pd.DataFrame(labels_check, columns=[\"label\"]).reset_index()\n",
    "        group = labels_check.groupby(['label']).count().reset_index().sort_values(by=['label'], ascending=True)\n",
    "        group['particip'] = (group['index'].values / sum(group['index'].values)) * 100\n",
    "        group.sort_values(by=['particip'], ascending=True, inplace=True)\n",
    "        group['label_name'] = group['label'].map(reverse_label_mapping)\n",
    "        \n",
    "        # Map standard colors for each label\n",
    "        group['color'] = group['label_name'].map(label_to_standard_color)\n",
    "        standard_plot_data[key] = group\n",
    "    \n",
    "    # Plot all clients with standard palette\n",
    "    cont = 0\n",
    "    for key, group in standard_plot_data.items():\n",
    "        ax = plt.subplot(1, num_clients, cont + 1)\n",
    "        \n",
    "        # Create bar plot with standard colors\n",
    "        y_pos = np.arange(len(group))\n",
    "        bars = ax.barh(y_pos, group['particip'].values, color=group['color'].values, \n",
    "                       edgecolor='black', linewidth=1.2, alpha=0.8)\n",
    "        \n",
    "        # Add percentage values on bars\n",
    "        for i, (y, value) in enumerate(zip(y_pos, group['particip'].values)):\n",
    "            ax.text(value + 1, y, f'{value:.1f}%', va='center', fontsize=9, \n",
    "                   fontweight='bold', color='#333333')\n",
    "        \n",
    "        # Customize axes\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(group['label_name'].values, fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Participation (%)', fontsize=10, fontweight='bold')\n",
    "        ax.set_title(f'{key.replace(\"_\", \" \")}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlim(left=0, right=max(group['particip']) + 12)\n",
    "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.set_facecolor('#fafafa')\n",
    "        \n",
    "        cont += 1\n",
    "    \n",
    "    # Overall figure styling\n",
    "    fig.suptitle('Client Label Distribution - Standard Color Palette', \n",
    "                fontsize=16, fontweight='bold', y=1.00)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ================================================================================\n",
    "    # DISPLAY STANDARD PALETTE INFORMATION\n",
    "    # ================================================================================\n",
    "    print(\"\\n[STANDARD PALETTE] Using 'husl' colormap from seaborn\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"✓ Number of classes: {len(label_mapping)}\")\n",
    "    print(f\"✓ Palette: HLS (Hue, Lightness, Saturation) - Professional & Accessible\")\n",
    "    print(f\"\\nColor assignments:\")\n",
    "    for label in sorted(label_mapping.keys()):\n",
    "        color_rgb = label_to_standard_color[label]\n",
    "        color_hex = '#{:02x}{:02x}{:02x}'.format(\n",
    "            int(color_rgb[0]*255), int(color_rgb[1]*255), int(color_rgb[2]*255)\n",
    "        )\n",
    "        print(f\"  • {label:<15} → {color_hex}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"✓ Visualization complete using standard professional color palette\")\n",
    "    print(\"=\"*100 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b191684",
   "metadata": {},
   "source": [
    "# Train with Dense Neural Network (DNN)\n",
    "\n",
    "Up to this point, we have shown how to use the **FedArtML** to create a federated dataset starting from centralized data. Then, we introduce the code to train an FL model using the **Flower framework**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c644cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "if not df.empty:\n",
    "    # Define loss and metrics\n",
    "    loss_inic = SparseCategoricalCrossentropy()\n",
    "    metrics = SparseCategoricalAccuracy()\n",
    "    \n",
    "    # Training configuration\n",
    "    epochs = 2\n",
    "    comms_round = 2\n",
    "    \n",
    "    print(\"\\nTraining Configuration:\")\n",
    "    print(f\"  - Local epochs per round: {epochs}\")\n",
    "    print(f\"  - Communication rounds: {comms_round}\")\n",
    "    print(f\"  - Number of clients: {local_nodes_glob}\")\n",
    "    print(f\"  - Input shape: {input_shape}\")\n",
    "    print(f\"  - Output classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72297c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # The `evaluate` function will be by Flower called after every round\n",
    "    def evaluate_DNN(\n",
    "        server_round: int,\n",
    "        parameters: fl.common.NDArrays,\n",
    "        config: Dict[str, fl.common.Scalar],\n",
    "    ) -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "        net = DNN_model(input_shape, num_classes)\n",
    "        net.set_weights(parameters) # Update model with the latest parameters\n",
    "        loss, accuracy, precision, recall, f1score  = test_model(net, x_test_glob, np.array(y_test_glob))\n",
    "        print(f\"@@@@@@ Server-side evaluation loss {loss} / accuracy {accuracy} / f1score {f1score} @@@@@@\")\n",
    "        return loss, {\"accuracy\": accuracy,\"precision\": precision,\"recall\": recall,\"f1score\": f1score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Starting Federated Learning Training\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define client function\n",
    "    def client_fn(cid: str) -> fl.client.Client:\n",
    "        # Create model\n",
    "        model = DNN_model(input_shape, num_classes)\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss=loss_inic, metrics=[metrics])\n",
    "        \n",
    "        # Load client data partition\n",
    "        x_train_cid = np.array(list_x_train[int(cid)], dtype=float)\n",
    "        y_train_cid = np.array(list_y_train[int(cid)], dtype=float)\n",
    "        \n",
    "        return FlowerClient(model, x_train_cid, y_train_cid, x_test_glob, y_test_glob, epochs)\n",
    "    \n",
    "    # Create FedAvg strategy\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=local_nodes_glob,\n",
    "        min_evaluate_clients=local_nodes_glob // 2,\n",
    "        min_available_clients=local_nodes_glob,\n",
    "        evaluate_fn=evaluate_DNN\n",
    "    )\n",
    "    \n",
    "    # Run federated learning simulation\n",
    "    start_time = time.time()\n",
    "    commun_metrics_history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=local_nodes_glob,\n",
    "        config=fl.server.ServerConfig(num_rounds=comms_round),\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Extract final metrics\n",
    "    global_acc_test = retrieve_global_metrics(commun_metrics_history, \"centralized\", \"accuracy\", False)\n",
    "    global_pre_test = retrieve_global_metrics(commun_metrics_history, \"centralized\", \"precision\", False)\n",
    "    global_rec_test = retrieve_global_metrics(commun_metrics_history, \"centralized\", \"recall\", False)\n",
    "    global_f1s_test = retrieve_global_metrics(commun_metrics_history, \"centralized\", \"f1score\", False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Accuracy:  {global_acc_test:.4f}\")\n",
    "    print(f\"Precision: {global_pre_test:.4f}\")\n",
    "    print(f\"Recall:    {global_rec_test:.4f}\")\n",
    "    print(f\"F1-Score:  {global_f1s_test:.4f}\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b57e07",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc22f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Define metrics to plot\n",
    "    metrics_show = [\"accuracy\",\"precision\",\"recall\",\"f1score\"]\n",
    "    metric_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "    # Create a larger figure with better spacing\n",
    "    fig = plt.figure(figsize=(24, 7))\n",
    "    fig.suptitle('Federated Learning Performance Metrics', fontsize=28, fontweight='bold', y=1.00)\n",
    "\n",
    "    # Loop over the communication round history and metrics\n",
    "    for i in range(len(metrics_show)):\n",
    "        ax = plt.subplot(1, len(metrics_show), i + 1)\n",
    "        \n",
    "        # Get metric data\n",
    "        metric_dict = commun_metrics_history.metrics_centralized\n",
    "        rounds, values = zip(*metric_dict[metrics_show[i]])\n",
    "        \n",
    "        # Plot with enhanced styling\n",
    "        ax.plot(np.asarray(rounds), np.asarray(values), color=colors[5], linewidth=4, marker='o', markersize=10, label='Test')\n",
    "        ax.fill_between(np.asarray(rounds), np.asarray(values), alpha=0.2, color=colors[5])\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_xlabel('Communication Round', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel(metric_labels[i], fontsize=16, fontweight='bold')\n",
    "        ax.set_title(metric_labels[i], fontsize=18, fontweight='bold', pad=15)\n",
    "        ax.set_ylim(-0.05, 1.08)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(fontsize=14, loc='lower right')\n",
    "        ax.tick_params(labelsize=13)\n",
    "        ax.set_axisbelow(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Define metrics in a dataframe\n",
    "    metrics_DNN = {'metric_name':  ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "               'set_used': ['Test', 'Test', 'Test', 'Test'],\n",
    "               'metric_value': [ global_acc_test , global_pre_test , global_rec_test , global_f1s_test]\n",
    "            }\n",
    "\n",
    "    metrics_DNN = pd.DataFrame(metrics_DNN, columns = ['metric_name', 'set_used','metric_value'])\n",
    "\n",
    "    # Plot metrics with improved styling\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            name='Test',\n",
    "            x=metrics_DNN[metrics_DNN['set_used']==\"Test\"]['metric_name'],\n",
    "            y=metrics_DNN[metrics_DNN['set_used']==\"Test\"]['metric_value'],\n",
    "            marker_color='#0091ad',\n",
    "            marker_line_color='#005f7a',\n",
    "            marker_line_width=2,\n",
    "            text=[f\"{v:.4f}\" for v in metrics_DNN[metrics_DNN['set_used']==\"Test\"]['metric_value']],\n",
    "            textposition='outside',\n",
    "            textfont=dict(size=14, color='#005f7a')\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Update layout for better appearance\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text='<b>Final Federated Learning Model Performance</b>',\n",
    "            font=dict(size=22, color='#005f7a'),\n",
    "            x=0.5,\n",
    "            xanchor='center'\n",
    "        ),\n",
    "        xaxis_title='<b>Metric</b>',\n",
    "        yaxis_title='<b>Metric Value</b>',\n",
    "        xaxis_title_font=dict(size=16),\n",
    "        yaxis_title_font=dict(size=16),\n",
    "        xaxis_tickfont=dict(size=14),\n",
    "        yaxis_tickfont=dict(size=14),\n",
    "        barmode='group',\n",
    "        autosize=False,\n",
    "        width=900,\n",
    "        height=600,\n",
    "        showlegend=False,\n",
    "        plot_bgcolor='rgba(240,240,240,0.5)',\n",
    "        margin=dict(l=100, r=100, t=100, b=100),\n",
    "        yaxis=dict(range=[0, 1.1])\n",
    "    )\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
